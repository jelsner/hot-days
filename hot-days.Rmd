---
title: "Hot Days in Tallahassee"
author: "James Elsner and Svetoslava Elsner"
date: "5/16/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

### The hots are getting hotter: A statistical analysis of extremely hot days in Tallahassee, FL, USA: 1940-2018

About the climate in Tallahassee.
http://weatherspark.com/history/31772/2013/Tallahassee-Florida-United-States
http://www.fao.org/geonetwork/srv/en/main.home

WSO weather station site: current location (since 1996) 30.39306°, -84.35333°
Downtown weather station site location 30.43333°, -84.28333°
Compare with Albany GA?

Raster map. Close up.
```{r}
Sites.df <- data.frame(Name = c("Regional Airport", "Downtown", 
                                "Dale Mabry Field", "Municipal Airport"), 
                       Latitude = c(30.39306, 30.43333, 30.44, 30.38),
                       Longitude = c(-84.35333, -84.28333, -84.338, -84.37))
```

```{r}
library(ggmap)

register_google(key = "")
getOption("ggmap")

geocode("Tallahassee Airport")

g_map <- get_googlemap(center = c(lon = -84.325, lat = 30.41),
                    zoom = 13, scale = 2,
                    maptype = 'satellite',
                    color = 'color')

cds <- attr(g_map, "bb")

bbx <- st_sfc(st_polygon(list(matrix(c(cds$ll.lon, cds$ur.lat,
                     cds$ur.lon, cds$ur.lat,
                     cds$ur.lon, cds$ll.lat,
                     cds$ll.lon, cds$ll.lat,
                     cds$ll.lon, cds$ur.lat), ncol = 2, byrow = TRUE))),
              crs = 4326)

p <- ggmap(g_map)

mainMap <- p + 
    geom_point(aes(x = Longitude, y = Latitude), 
               data = Sites.df, size = 2, col = c("orange", "red", "orange", "orange")) +
    geom_label(aes(x = Longitude, y = Latitude, label = Name), 
             data = Sites.df,
             nudge_y = -.0025,
             nudge_x = .005) +
    theme_void() +
    theme(legend.position = "none")
```

Create a map with an inset showing Florida and Leon County.
```{r}
library(sf)
library(dplyr)

library(tmap)
library(USAboundaries)

FL.sf <- us_states(states = "Florida")
FL.sf$name <- "            Florida"
ALGA.sf <- us_states(states = c("Alabama", "Georgia"))

Leon.sf <- us_counties(states = "FL") %>%
  filter(name == "Leon")

inset.tm <- 
    tm_shape(FL.sf, projection = "laea_NA") +
       tm_polygons(col = "white") +
       tm_text(text = "name", size = 1) +
    tm_shape(ALGA.sf) +
       tm_polygons(col = "gray") +
    tm_shape(Leon.sf) +
       tm_borders() +
    tm_shape(bbx) +
       tm_polygons(col = "black") +
    tm_compass(type = "arrow", position = c("left", "bottom")) +
    tm_scale_bar(position = c("left", "bottom"), size = .55) +
    tm_style("natural")

library(grid)

mainMap
print(inset.tm, vp = viewport(.75, .25, width = .7, height = .4))
```
###Figure: Satellite image showing the locations of the sites where daily temperature records were taken in the city of Tallahassee. Inset: Leon County, Florida. The black square inside the county is the area of the background satellite image.



Get the data. Ordered online from 
https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00093805/detail
 Elev: 55 ft. Lat: 30.3931° N Lon: -84.3533° W

TALLAHASSEE REGIONAL AIRPORT, FL US (Station ID: GHCND:USW00093805)
TALLAHASSEE, FL US (Station ID: GHCND:USC00088754)

Sofia Bulgaria
https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:BUM00015614/detail

https://docs.opendata.aws/noaa-ghcn-pds/readme.html

## Hot days and nights

Get data.
```{r}
library(lubridate)

TLH.df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) %>%
      filter(STATION == 'USW00093805') %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), 
             Month = month(Date), 
             Day = day(Date),
             doy = yday(Date)) %>%
      select(Date, Year, Month, Day, doy, TMAX, TMIN, PRCP)
```

Add temperatures for July 8, 2005 from Weather Underground.
```{r}
TLH.df$Date[is.na(TLH.df$TMAX)]
TLH.df$Date[is.na(TLH.df$TMIN)]

TLH.df$TMAX[TLH.df$Date == "2005-07-08"] <- 96
TLH.df$TMIN[TLH.df$Date == "2005-07-08"] <- 71
TLH.df$TMIN[TLH.df$Date == "2002-10-08"] <- TLH.df$TMIN[TLH.df$Date == "2002-10-07"]
```

Histogram of daily high temperature.
```{r}
FreqByT <- TLH.df %>%
#  filter(Month >= 5 & Month <= 9) %>%
#    filter(Year < 1980) %>%
  group_by(TMAX) %>%
  summarize(nH = n())
FreqByT2 <-FreqByT %>%
  filter(TMAX >= 100)

( p1 <- ggplot(FreqByT, aes(x = TMAX, y = nH)) +
  geom_bar(stat = 'identity', col = 'white', fill = "gray70") +
  geom_bar(data = FreqByT2, stat = 'identity', fill = "orange") +
  scale_x_continuous(breaks = seq(40, 100, 10),
                     labels = paste0(seq(40, 100, 10), "°")) +
  scale_y_continuous(position = "right", limits = c(0, 1600)) +
  ylab("") +  xlab("") +
#  ggtitle(label = "Frequency of Daily High Temperatures",
#          subtitle = "Tallahassee, FL, USA (1940-2018)") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) )

FreqByT <- TLH.df %>%
#    filter(Month >= 5 & Month <= 9) %>%
#  filter(Year < 1980) %>%
  group_by(TMIN) %>%
  summarize(nH = n())
FreqByT2 <-FreqByT %>%
  filter(TMIN >= 77)

(p2 <- ggplot(FreqByT, aes(x = TMIN, y = nH)) +
  geom_bar(stat = 'identity', col = 'white', fill = "gray70") +
  geom_bar(data = FreqByT2, stat = 'identity', fill = "orange") +
  scale_x_continuous(breaks = seq(20, 80, 10),
                     labels = paste0(seq(20, 80, 10), "°")) +
  scale_y_continuous(position = "right", limits = c(0, 1600)) +
  ylab("") + xlab("") +
#  ggtitle(label = "Frequency of Daily Low Temperatures",
#          subtitle = "Tallahassee, FL, USA (1940-2018)") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) )

library(ggpubr)

ggarrange(p1, p2, 
          nrow = 2, 
          labels = c("Number of Days by Maximum Temperature", 
                     "Number of Days by Minimum Temperature"),
          font.label = list(face = "plain", size = 11))
```
###Figure: The number of days by high temperature (top) and by low temperature (bottom).

How many hot days/nights? Median date. Seasonal variability.
```{r}
TLH.df %>%
  summarize(nD = n(),
            N100 = sum(TMAX >= 100),
            N77 = sum(TMIN >= 77),
            qtle100 = 1 - N100/nD,
            qtle77 = 1 - N77/nD)

TLH.df %>%
  summarize(N98 = sum(TMAX == 98),
            N99 = sum(TMAX == 99),
            N100 = sum(TMAX == 100),
            N101 = sum(TMAX == 101),
            N75 = sum(TMIN == 75),
            N76 = sum(TMIN == 76),
            N77 = sum(TMIN == 77),
            N78 = sum(TMIN == 78))
```

The 99.555th percentile of the distribution of daily high temperatures in the observations. 4 days in 1000. The 99.4368th percentile of the distribution of daily low temperatures.

Monthly counts.
```{r}
library(xtable)
TLH.df %>%
  group_by(Month) %>%
  summarize(N100 = sum(TMAX >= 100, na.rm = TRUE),
            N77 = sum(TMIN >= 77, na.rm = TRUE)) %>%
  filter(Month %in% 5:10) %>%
  mutate(MonthName = month.name[Month]) %>%
  dplyr::select(MonthName, N100, N77) %>%
  xtable()
  
```
###Table: Monthly counts.

Seasonality. 
```{r}
TLH.df %>%
  filter(TMIN >= 77) %>%
  arrange(desc(doy)) %>%
  head

TLH.df %>%
  filter(TMAX >= 100) %>%
  summarize(medianDate = median(doy))

# 182: July 1st
TLH.df %>%
  filter(TMIN >= 77) %>%
  summarize(medianDate = median(doy))
# 216: August 4th

library(reshape2)

DOY <- TLH.df %>%
  group_by(doy) %>%
  summarize("Number of Times the Daytime High Reached 100° F" = sum(TMAX >= 100, na.rm = TRUE),
            "Number of Times the Nighttime Low Remained Above 77° F" = sum(TMIN >= 77, na.rm = TRUE)) %>%
  filter(doy %in% 139:250) %>%
  melt(id.vars = "doy")

ggplot(DOY, aes(x = doy, y = value)) +
  geom_bar(stat = 'identity', col = "white", fill = "orange") +
  scale_x_continuous(position = "bottom", 
                     breaks = c(152, 166, 182, 196, 213, 227, 244),
                     labels = c("June 1", "June 15", 
                                "July 1", "July 15", 
                                "August 1", "August 15",
                                "September 1")) +
  scale_y_continuous(position = "left") +
  facet_wrap(~ variable, nrow = 2, strip.position = "top") +
  ylab("") + xlab("") +
#  ggtitle(label = "Number of Extremely Hot Days & Nights By Day of Year",
#          subtitle = "Tallahassee, Florida, USA (1940-2018)") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank(),
        strip.text.x = element_text(size = 11),
        legend.position = "none") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50")) 
```
###Figure: Number of times by day of the year the daily high reached 100F (top) and number of times the daily low remained at or above 77F.

NOT USED
```{r}
dailyRecs <- TLH.df %>% 
  group_by(Month, Day) %>%
  summarize(dMaxMax = max(TMAX, na.rm=TRUE),
            dMaxMin = max(TMIN, na.rm=TRUE),
            dMaxMaxY = Year[which.max(TMAX)],
            dMaxMinY = Year[which.max(TMIN)])

dailyRecs %>%
  group_by(dMaxMaxY) %>%
  summarize(count = length(dMaxMaxY)) %>%
ggplot(aes(x = dMaxMaxY, y = count)) +
  geom_bar(stat = 'identity') +
  ylab("Number of dates having the daily record high maximum")

dailyRecs %>%
  group_by(dMaxMinY) %>%
  summarize(count = length(dMaxMinY)) %>%
ggplot(aes(x = dMaxMinY, y = count)) +
  geom_bar(stat = 'identity') +
  ylab("Number of dates having the daily record high minimum")
```

Annual time series of extreme counts.
```{r}
library(MASS)
TLH.df %>% 
  group_by(Year) %>%
  summarize("High Temperature At Or Above 100°"  = sum(TMAX >= 100, na.rm = TRUE),
            "Low Temperature At Or Above 77°" = sum(TMIN >= 77, na.rm = TRUE)) %>%
  melt(id.vars = "Year") %>%
ggplot(aes(x = Year, y = value, fill = value)) + 
  geom_bar(stat='identity') + 
     stat_smooth(method = "glm.nb",
                 formula = y ~ x, 
                 se = FALSE,
                 col = "green") +
  scale_fill_continuous(low = 'orange', high = 'red') +
  scale_x_continuous(position = "top", breaks = seq(1945, 2015, by = 10)) +
  scale_y_continuous(limits = c(0, 25), position = "right") +
#  geom_text(aes(label = value), vjust = -.5, size = 2.5) +
  facet_wrap(~ variable, nrow = 2, strip.position = "bottom") +
  ylab("") + xlab("") +
  ggtitle(label = "Number of Extremely Hot Days & Nights",
          subtitle = "Tallahassee, Florida, USA (1940-2018)") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        strip.text.x = element_text(size = 11),
        legend.position = "none") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50")) 

TLH.df %>% 
  group_by(Year) %>%
  summarize(N100 = sum(TMAX >= 100)) %>%
  arrange(desc(N100))

TLH.df %>% 
  group_by(Year) %>%
  summarize(N77 = sum(TMIN >= 77)) %>%
  arrange(desc(N77))
```
### Figure: Time series of the number of extremely hot days and nights.

Quantify the trend.
```{r}
Yearly.df <- TLH.df %>% 
  group_by(Year) %>%
  summarize(MaxCount  = sum(TMAX >= 100),
            MinCount = sum(TMIN >= 77))

Yearly.df %>%
  summarize(avgCountMax = mean(MaxCount),
            avgCountMin = mean(MinCount),
            varCountMax = var(MaxCount),
            varCountMin = var(MinCount),
            ratioMax = varCountMax/avgCountMax,
            ratioMin = varCountMin/avgCountMin)

var(Yearly.df$MaxCount)/mean(Yearly.df$MaxCount)
var(Yearly.df$MinCount)/mean(Yearly.df$MinCount)

exp(coef(glm.nb(MaxCount ~ 1, data = Yearly.df)))  # 1.62 =  annual rate of 100+F days
exp(coef(glm.nb(MaxCount ~ Year, data = Yearly.df))) # 1.02% or 2% per year increase in annual rate

exp(coef(glm.nb(MinCount ~ 1, data = Yearly.df))) # 2.05  annual rate of 77+F nights
exp(coef(glm.nb(MinCount ~ Year, data = Yearly.df))) # 1.045 or 4.5% per year increase in annual rate

fitNBMax <- glm.nb(MaxCount ~ Year, data = Yearly.df)
fitPO <- glm(MaxCount ~ Year, data = Yearly.df, family = "poisson")
AIC(fitNBMax); AIC(fitPO)

pchisq(2 * (logLik(fitNBMax) - logLik(fitPO)), df = 1, lower.tail = FALSE)

(exp(summary(fitNBMax)$coefficients[2]) - 1) * 100  # Trend (%/yr)
(exp(summary(fitNBMax)$coefficients[4]) - 1) * 100   # Margin of error (%/yr)

predict(fitNBMax, newdata = data.frame(Year = 1940), type = "response", se = TRUE)
predict(fitNBMax, newdata = data.frame(Year = 2018), type = "response", se = TRUE)
```

For the negative binomial.
```{r}
pnbinom(0, size = 1, mu = .6278)  # probabilty of no hot days when the mean is .22
(1 - pnbinom(0:4, size = 1, mu = .6278)) * 100 # probability of at least 1, at least 2, ...
(1 - pnbinom(0:4, size = 1, mu = 3.235707)) * 100
```

```{r}
fitNBMin <- glm.nb(MinCount ~ Year, data = Yearly.df)

(exp(summary(fitNBMin)$coefficients[2]) - 1) * 100  # Trend (%/yr)
(exp(summary(fitNBMin)$coefficients[4]) - 1) * 100   # Margin of error (%/yr)

predict(fitNBMin, newdata = data.frame(Year = 1940), type = "response", se = TRUE)
predict(fitNBMin, newdata = data.frame(Year = 2018), type = "response", se = TRUE)

(1 - pnbinom(0:4, size = 1, mu = .22093)) * 100 # probability of at least 1, at least 2, ...
(1 - pnbinom(0:4, size = 1, mu = 6.77268)) * 100
```

### Hot events

Determine hot events (day & night).
```{r}
hotDayEvents <- rle(TLH.df$TMAX >= 100)
hotNightEvents <- rle(TLH.df$TMIN >= 77)

dayEventLength <- hotDayEvents$lengths[hotDayEvents$values]
dayEventNo <- rep(1:length(dayEventLength), dayEventLength)

nightEventLength <- hotNightEvents$lengths[hotNightEvents$values]
nightEventNo <- rep(1:length(nightEventLength), nightEventLength)

dayEvents.df <- TLH.df %>%
  filter(TMAX >= 100) %>%
  mutate(dayEventNo = dayEventNo)

nightEvents.df <- TLH.df %>%
  filter(TMIN >= 77) %>%
  mutate(nightEventNo = nightEventNo)
```

Determine the number of days between successive 100+F days. Add this as another column. Repeat for nights between successive 77+F nights.
```{r}
dayEvents.df <- dayEvents.df %>%
  mutate(daysBetween = Date - lag(Date))

dayEvents.df %>%
  summarize(totalNoEvents = last(dayEventNo),
            maxTimeBetween = max(daysBetween, na.rm = TRUE),
            minTimeBetween = min(daysBetween, na.rm = TRUE),
            medianTimeBetween = median(daysBetween, na.rm = TRUE))

nightEvents.df <- nightEvents.df %>%
  mutate(nightsBetween = Date - lag(Date))

nightEvents.df %>%
  summarize(totalNoEvents = last(nightEventNo),
            maxTimeBetween = max(nightsBetween, na.rm = TRUE),
            minTimeBetween = min(nightsBetween, na.rm = TRUE),
            medianTimeBetween = median(nightsBetween, na.rm = TRUE))
```

Length and intensity of events.
```{r}
LIdayEvents.df <- dayEvents.df %>%
  group_by(dayEventNo) %>%
  summarize(eventLength = n(),
            avgEventT = mean(TMAX),
            maxEventT = max(TMAX),
            whenMaxEvT = which.max(TMAX),
            Year = Year[1])
LInightEvents.df <- nightEvents.df %>%
  group_by(nightEventNo) %>%
  summarize(eventLength = n(),
            avgEventT = mean(TMIN),
            maxEventT = max(TMIN),
            whenMaxEvT = which.max(TMIN),
            Year = Year[1])

LIdayEvents.df %>%
  summarize(avgEventLength = mean(eventLength),
            medEventLength = median(eventLength),
            maxEventLength = max(eventLength),
            whenMaxEventLength = which.max(eventLength))
dayEvents.df$Date[dayEvents.df$dayEventNo == 39]

LInightEvents.df %>%
  summarize(avgEventLength = mean(eventLength),
            medEventLength = median(eventLength),
            maxEventLength = max(eventLength),
            whenMaxEventLength = which.max(eventLength))
nightEvents.df$Date[nightEvents.df$nightEventNo == 100]

# empirical chance of another hot day 
sum(LIdayEvents.df$eventLength > 1)/length(LIdayEvents.df$eventLength) # 41.4%
sum(LInightEvents.df$eventLength > 1)/length(LInightEvents.df$eventLength) # 23.3%

cor(LIdayEvents.df$eventLength, LIdayEvents.df$maxEventT, method = "s") # .55
cor(LInightEvents.df$eventLength, LInightEvents.df$maxEventT, method = "s") # .54

```

Relationships between event length and maximum event temperature.
```{r}
library(lme4)

summary(lm(eventLength ~ Year, data = LIdayEvents.df))
summary(lm(maxEventT ~ eventLength, data = LIdayEvents.df))

summary(lm(eventLength ~ Year, data = LInightEvents.df))
summary(lm(maxEventT ~ eventLength, data = LInightEvents.df))
```


NOT SURE WE WANT TO DO IT THIS WAY. WHY NOT MODEL THE EVENT LENGTHS
```{r}
LIdayEvents2.df <- LIdayEvents.df %>%
  group_by(Year) %>%
  summarize(count = n(),
            avgEL = mean(eventLength))
LInightEvents2.df <- LInightEvents.df %>%
  group_by(Year) %>%
  summarize(count = n(),
            avgEL = mean(eventLength))

AllYears <- data.frame(Year = 1940:2018)

LIdayEvents2.df <- merge(AllYears, LIdayEvents2.df, by = "Year", all.x = TRUE)
LIdayEvents2.df$count[is.na(LIdayEvents2.df$count)] <- 0

LInightEvents2.df <- merge(AllYears, LInightEvents2.df, by = "Year", all.x = TRUE)
LInightEvents2.df$count[is.na(LInightEvents2.df$count)] <- 0

var(LIdayEvents2.df$count)/mean(LIdayEvents2.df$count) # 2.46
var(LInightEvents2.df$count)/mean(LInightEvents2.df$count) # 3.91
```

Plot trends in frequency and duration.
```{r}
p1 <- ggplot(data = LIdayEvents2.df, aes(x = Year, y = count)) +
   geom_point(col = "gray70") +
#   stat_smooth(method = "glm.nb",
#              formula = y ~ x, 
#              data = LIdayEvents2.df, se = TRUE) +
  scale_x_continuous(position = "bottom", breaks = seq(1945, 2015, by = 10)) +
  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 2), position = "left") +
  ylab("") + xlab("") +
  theme_minimal() +
    ggtitle("Number of daytime hot events") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(size = 11))

p2 <- ggplot(data = LIdayEvents2.df, aes(x = Year, y = avgEL)) +
   geom_bar(stat = "identity", fill = "gray70") +
#  stat_smooth(method = "glm",
#               method.args = list(family = "Gamma"),
#               formula = y ~ x, 
#               se = TRUE)
  scale_x_continuous(position = "bottom", breaks = seq(1945, 2015, by = 10)) +
  scale_y_continuous(position = "left", breaks = seq(1, 8, 1)) +
  ylab("") + xlab("") +
  theme_minimal() +
    ggtitle("Average length (days) of daytime hot events") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(size = 11)) 

p3 <- ggplot(data = LInightEvents2.df, aes(x = Year, y = count)) +
  geom_point(col = "gray70") +
  scale_x_continuous(position = "bottom", breaks = seq(1945, 2015, by = 10)) +
  scale_y_continuous(limits = c(0, 14), breaks = seq(0, 14, 2), position = "left") +
  ylab("") + xlab("") +
  theme_minimal() +
    ggtitle("Number of nighttime hot events") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(size = 11)) 

p4 <- ggplot(data = LInightEvents2.df, aes(x = Year, y = avgEL)) +
   geom_bar(stat = "identity", fill = "gray70") +
  scale_x_continuous(position = "bottom", breaks = seq(1945, 2015, by = 10)) +
  scale_y_continuous(position = "left", breaks = seq(1, 8, 1)) +
  ylab("") + xlab("") +
  theme_minimal() +
    ggtitle("Average length (nights) of nighttime hot events") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(size = 11))

ggarrange(p1, p3, p2, p4, ncol = 2, nrow = 2)
```
###Figure: Frequency and duration of hot daytime and nighttime events.

###Hot day preceded or followed by a hot night
```{r}
TLH.df %>%
  filter(TMAX >= 100 & TMIN >= 77 | 
 #        TMAX >= 100 & lag(TMIN >= 77) |
         TMAX >= 100 & lead(TMIN >= 77)) 

TLH.df %>%
  filter(TMAX >= 100 & TMIN >= 77 | 
         TMAX >= 100 & lead(TMIN >= 77)) %>%
  group_by(Month) %>%
  summarize(nE = n())
```

1940-49 0
1950-59 3
1960-69 0
1970-79 0
1980-89 6
1990-99 4
2000-09 9
2010-18 11

### Analysis of temperatures when data are available at both sites

```{r}
ALL.df <- read.csv(file = 'TLH_SOD1892.csv',
                   stringsAsFactors = FALSE,
                   header = TRUE) %>%
        mutate(Date = as.Date(DATE))

ALL.df %>%
  filter(STATION == "USC00088754") %>%
  summarize(start = first(Date),
            end = last(Date))

duplicateRows <- duplicated(ALL.df$Date)
duplicateDates <- ALL.df$Date[duplicateRows]

df1 <- ALL.df %>%
  filter(Date %in% duplicateDates) %>%
  filter(STATION == "USC00088754")

df2 <- ALL.df %>%
  filter(Date %in% duplicateDates) %>%
  filter(STATION == "USW00093805") %>%
  dplyr::select(STATION2 = STATION, TMAX2 = TMAX, TMIN2 = TMIN, PRCP2 = PRCP)

OVR.df <- cbind(df1, df2)
```

TMAX2/TMIN2: Airport, TMAX/TMIN: Downtown
```{r}
OVR.df %>%
  summarize(avgTMAX = mean(TMAX, na.rm = TRUE),
            avgTMAX2 = mean(TMAX2, na.rm = TRUE),
            avgTMIN = mean(TMIN, na.rm = TRUE),
            avgTMIN2 = mean(TMIN2, na.rm = TRUE))

OVR.df %>%
  filter(TMAX >= 100) %>%
  summarize(avgTMAX = mean(TMAX, na.rm = TRUE),
            avgTMAX2 = mean(TMAX2, na.rm = TRUE),
            avgTMIN = mean(TMIN, na.rm = TRUE),
            avgTMIN2 = mean(TMIN2, na.rm = TRUE))

OVR.df %>%
  filter(TMIN >= 77) %>%
  summarize(avgTMAX = mean(TMAX, na.rm = TRUE),
            avgTMAX2 = mean(TMAX2, na.rm = TRUE),
            avgTMIN = mean(TMIN, na.rm = TRUE),
            avgTMIN2 = mean(TMIN2, na.rm = TRUE))

OVR.df %>%
  summarize(DTN_N100 = sum(TMAX >= 100, na.rm = TRUE),
            TLH_N100 = sum(TMAX2 >= 100, na.rm = TRUE),
            DTN_N77 = sum(TMIN >= 77, na.rm = TRUE),
            TLH_N77 = sum(TMIN2 >= 77, na.rm = TRUE))
          
```

Logistic regression model.
```{r}
lrm0 <- glm(I(TMAX2 >= 100) ~ 1, data = OVR.df, family = "binomial")
lrm1 <- glm(I(TMAX2 >= 100) ~ TMAX, data = OVR.df, family = "binomial")
predict(lrm, newdata = data.frame(TMAX = c(95:103)), type = "response")
AIC(lrm0)
AIC(lrm1)
```

Predictions.
```{r}
pred1 <- predict(lrm1, newdata = data.frame(TMAX = c(95:104)), type = "response") * 100 
data.frame(TMAX = c(95:104), PredProbs = pred1) %>%
  xtable()

pred2 <- predict(lrm1, newdata = data.frame(TMAX = c(102, 103)), type = "response")
```

What is the likely number of 100 degree days at the airport over the period of record when the daily high temperature was recorded downtown?
```{r}
DTN.df <- ALL.df %>%
  filter(STATION == "USC00088754" & Date < as.Date("1940-03-01"))

pred3 <- predict(lrm1, 
                 newdata = data.frame(TMAX = DTN.df$TMAX[DTN.df$TMAX >= 95]), 
                 type = "response")

counts <- numeric()
for(i in 1:length(pred3)){
counts <- c(counts, rbinom(n = 100, size = 1, prob = pred3[i]))
}
  
n100 <- numeric()
for(i in 1:100){
TMAX2 <- add_predicted_draws(newdata = data.frame(TMAX = DTN.df$TMAX, Date = DTN.df$Date), 
                    fit1, n = 1000) %>%
  mutate(TMAX2est = as.integer(.prediction) + minTMAX2 - 1) %>%
  group_by(Date) %>%
  sample_n(size = 1) %>%
  pull(TMAX2est) 
n100 <- c(n100, sum(TMAX2 >= 100))
}
```

Bayesian logistic regression.
```{r}
library(brms)

OVRhotDays.df <- OVR.df %>%
  filter(TMAX >= 94) %>%
  mutate(TDT = scale(TMAX),
         TI = as.integer(TMAX2 >= 100))

family <- brms::bernoulli
formula <- TI ~ 1

get_prior(formula,
          data = OVRhotDays.df, 
          family = family)

fit0 <- brm(formula = formula,
            data = OVRhotDays.df,
            family = family,
            prior = set_prior("student_t(3, 0, 10)", class = "Intercept"),
            seed = 9121)

fixef(fit0)

fit0_out <- predict(fit0)
head(fit0_out)

formula <- TI ~ TMAX
get_prior(formula, data = OVRhotDays.df, family = family)

fit1 <- brm(formula = formula,
            data = OVRhotDays.df,
            family = family,
            prior = c(set_prior("normal(0, 5)", class = "b"),
                     set_prior("student_t(3, 0, 10)", class = "Intercept")),
           seed = 711)

summary(fit1)

library(tidybayes)
library(modelr)

# initialize out data frame
out <- OVRhotDays.df %>%
  data_grid(TMAX = 96:106) %>%
  add_predicted_draws(fit1, n = 1000) %>%
  mutate(pTMAX2 = .prediction) %>%
  group_by(TMAX) %>%
  summarize(nTMAX = n(),
            prob100 = sum(pTMAX2)/nTMAX * 100)

out

# rbind to the initialized data frame
for(i in 2:100){
  out <- OVRhotDays.df %>%
  data_grid(TMAX = 96:106) %>%
  add_predicted_draws(fit1, n = 1000) %>%
  mutate(pTMAX2 = .prediction) %>%
  group_by(TMAX) %>%
  summarize(nTMAX = n(),
            prob100 = sum(pTMAX2)/nTMAX * 100) %>%
    rbind(out)
}

out$I <- rep(1:100, each = 11)

library(ggridges)

ggplot(out, aes(x = TMAX, y = prob100, group = I)) +
  geom_point(alpha = .2, size = .5) +
  scale_x_continuous(breaks = 96:106) +
  xlab("High Temperature Downtown Tallahassee (°F)") +
  ylab("Posterior Probability") +
  ggtitle("Chance WSO Tallahassee Reports a 100+°F Day",
          subtitle = "Given High Temperatures Downtown") +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank())
```
###Figure: Posterior probability of getting a 100+F day given the high temperature downtown.

Get the data from the downtown site. Filter by days at or above 96F then use the model to predict the temperature at the airport. August 31, 1895 TMAX = 2768 is removed. Remove days after the start of the airport record.
```{r}
DTN.df <- ALL.df %>%
      filter(STATION == 'USC00088754') %>%
      filter(TMAX >= 92) %>%
      filter(TMAX != 2768) %>%
      filter(Date < as.Date("1940-03-01")) %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      dplyr::select(Date, Year, Month, Day, TMAX, TMIN, PRCP) 
table(DTN.df$TMAX)

sum(DTN.df$TMAX >= 100)
```

Predictions. What is the likely number of 100 degree days at the airport over the period of record when the daily high temperature was recorded only downtown?
```{r}
n100 <- numeric()
for(i in 1:100){
HOTday <- add_predicted_draws(newdata = data.frame(TMAX = DTN.df$TMAX, Date = DTN.df$Date), 
                    fit1, n = 1000) %>%
  mutate(HOTday = .prediction) %>%
  group_by(Date) %>%
  sample_n(size = 1) %>%
  pull(HOTday) 
n100 <- c(n100, sum(HOTday))
}

ggplot(as.data.frame(n100), aes(n100)) +
  geom_bar(col = 'gray70', fill = "gray70") +
  xlab("Number of Days") + ylab("Frequency") +
  theme_minimal()
```

Simulate counts using the model over early years. Then concatenate the later years. 
```{r}
N = 3
p_draws <- add_predicted_draws(newdata = data.frame(TMAX = DTN.df$TMAX, Date = DTN.df$Date), 
                               fit1, n = N) %>%
  mutate(Year = year(Date)) %>%
  group_by(.draw, Year) %>%
  summarize(nHotDays = sum(.prediction > 0))

YC_sim.df <- data.frame(Year = seq(1892, 1939, 1))

YC_sim.df <- left_join(YC_sim.df, p_draws, by = "Year")

YC1.df <- ALL.df %>%
      filter(STATION == 'USC00088754') %>%
      mutate(Date = as.Date(DATE),
             Year = year(Date)) %>%
      group_by(Year) %>%
      summarize(.draw = 0,
                nHotDays = sum(TMAX >= 100, na.rm = TRUE)) %>%
      filter(Year < 1940)

YC2.df <- data.frame()
for(i in 0:N){
YC2.df <- ALL.df %>%
      filter(STATION == 'USW00093805') %>%
      mutate(Date = as.Date(DATE),
             Year = year(Date)) %>%
      group_by(Year) %>%
      summarize(.draw = i,
                nHotDays = sum(TMAX >= 100)) %>%
  rbind(YC2.df)
}

YC.df <- rbind(YC_sim.df, YC1.df, YC2.df)

YC.df %>%
  group_by(.draw) %>%
  summarize(nYears = n())
```

Compute the trends. Plot a density of the trends. Add the raw value.
```{r}
library(broom)

YC.df %>% 
  group_by(.draw) %>%
  do(tidy(glm.nb(nHotDays ~ Year, data = .))) %>%
  filter(term == "Year") %>%
  mutate(Trend = (exp(estimate) - 1) * 100)

# Create a density plot of these trends
```




Predictions. No loop. Look at three realizations from the model. Join predictions data frame with the original downtown data frame.
```{r}
x <- add_predicted_draws(newdata = data.frame(TMAX = DTN.df$TMAX, Date = DTN.df$Date), 
                    fit1, n = 1000) %>%
  mutate(HOTday = .prediction) %>%
  group_by(Date) %>%
  sample_n(size = 3) %>%
  pull(HOTday)

z <- data.frame(matrix(x, ncol = 3, byrow = FALSE))
names(z) <- c("C1", "C2", "C3")
z$Date <- DTN.df$Date

YearCount.df <- ALL.df %>%
      filter(STATION == 'USC00088754') %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      dplyr::select(Date, Year, Month, Day, TMAX, TMIN, PRCP) %>%
left_join(z, by = "Date") %>%
        filter(Year < 1940) %>%
        group_by(Year) %>%
        summarize(C1 = sum(C1, na.rm = TRUE),
                  C2 = sum(C2, na.rm = TRUE),
                  C3 = sum(C3, na.rm = TRUE),
                  Uncorrected = sum(TMAX >= 100, na.rm = TRUE))
```

Get only the data from the airport site.
```{r}
TLH.df <- ALL.df %>%
      filter(STATION == 'USW00093805') %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      dplyr::select(Date, Year, Month, Day, TMAX, TMIN, PRCP)
```

```{r}
YearCount2.df <- TLH.df %>%
  group_by(Year) %>%
  summarize(C1 = sum(TMAX >= 100, na.rm = TRUE),
            C2 = sum(TMAX >= 100, na.rm = TRUE),
            C3 = sum(TMAX >= 100, na.rm = TRUE),
            Uncorrected = C1)

library(reshape2)
YC.df <- rbind(YearCount.df, YearCount2.df) %>%
  melt(id.vars = "Year") %>%
  mutate(variable = factor(variable, 
                           levels = c("Uncorrected", 
                                      "C1", "C2", "C3")))
levels(YC.df$variable) <- c("Raw", "Corrected 1", "Corrected 2", "Corrected 3")

ggplot(YC.df, aes(x = Year, y = value, fill = value)) + 
  geom_bar(stat='identity') + 
  scale_fill_continuous(low = 'orange', high = 'red') +
  geom_text(aes(label = value), vjust = -.5, size = 3) +
  scale_y_continuous(limits = c(0, 25), position = "right") +
  ylab("Number of Days") +
  facet_wrap(~ variable, nrow = 4, strip.position =  "bottom" ) +
  ggtitle(label = expression(paste('Number of days at or above 100', {}^o, " F")),
          subtitle = "Tallahassee, Florida, USA (1892-2018)") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 11), 
        legend.position = "none") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50"))
```












Cumulative logistic regression model. Note: The model cannot predict temperatures above 100 at the airport since there were none in the record during the overlapping years. It can assign an exceedance probability for a larger range of input temperatures.
```{r}
library(brms)

OVRhotDays.df <- OVR.df %>%
  filter(TMAX >= 94) %>%
  mutate(minTMAX2 = min(TMAX2),
         TI = TMAX2 - minTMAX2 + 1,
         TDT = scale(TMAX))

family <- brms::cumulative(threshold = "equidistant")
formula <- TI ~ 1

get_prior(formula,
          data = OVRhotDays.df, 
          family = family)

fit0 <- brm(formula = formula,
            data = OVRhotDays.df,
            family = family,
            prior = set_prior("student_t(3, 0, 10)", class = "Intercept"),
            seed = 9121)

fixef(fit0)

fit0_out <- predict(fit0, probs = c(0, 1))
head(fit0_out)

formula <- TI ~ TMAX

get_prior(formula, data = dfModel2, family = family)

fit1 <- brm(formula = formula,
            data = dfModel2,
            family = family,
            prior = c(set_prior("normal(0, 1)", class = "b"),
                     set_prior("student_t(7, 0, 10)", class = "Intercept")),
           seed = 78121)

summary(fit1)

library(tidybayes)
library(modelr)

# initialize out data frame
out <- dfModel2 %>%
  data_grid(TMAX = 96:106) %>%
  add_predicted_draws(fit1, n = 1000) %>%
  mutate(pTMAX2 = as.integer(.prediction)  + minTMAX2 - 1) %>%
  group_by(TMAX) %>%
  summarize(nTMAX = n(),
            prob100 = sum(pTMAX2 >= 100)/nTMAX) 

out

# rbind to the initialized data frame
for(i in 2:100){
  out <- dfModel2 %>%
  data_grid(TMAX = 96:106) %>%
  add_predicted_draws(fit1, n = 1000) %>%
  mutate(pTMAX2 = as.integer(.prediction)  + minTMAX2 - 1) %>%
  group_by(TMAX) %>%
  summarize(nTMAX = n(),
            prob100 = sum(pTMAX2 >= 100)/nTMAX) %>%
    rbind(out)
}

out$I <- rep(1:100, each = 11)

library(ggridges)

ggplot(out, aes(x = TMAX, y = prob100, group = I)) +
  geom_point(alpha = .2, size = .5) +
  scale_x_continuous(breaks = 96:106) +
  xlab("High Temperature Downtown Tallahassee (°F)") +
  ylab("Posterior Probability") +
  ggtitle("Chance the TLH Airport Reports a 100+°F Day",
          subtitle = "Given specific high temperatures downtown") +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank())
```

Read only the data from the downtown site. Filter by days at or above 96F then use the model to predict the temperature at the airport. August 31, 1895 TMAX = 2768 is removed.
```{r}
DTN.df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) %>%
      filter(STATION == 'USC00088754') %>%
      filter(TMAX >= 96) %>%
      filter(TMAX != 2768) %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      select(Date, Year, Month, Day, TMAX, TMIN, PRCP) 
table(DTN.df$TMAX)
```

Predictions. What is the likely number of 100 degree days at the airport over the period of record when the daily high temperature was recorded downtown?
```{r}
n100 <- numeric()
for(i in 1:100){
TMAX2 <- add_predicted_draws(newdata = data.frame(TMAX = DTN.df$TMAX, Date = DTN.df$Date), 
                    fit1, n = 1000) %>%
  mutate(TMAX2est = as.integer(.prediction) + minTMAX2 - 1) %>%
  group_by(Date) %>%
  sample_n(size = 1) %>%
  pull(TMAX2est) 
n100 <- c(n100, sum(TMAX2 >= 100))
}
```

Predictions. No loop. Look at three realizations from the model. Join predictions data frame with the original downtown data frame.
```{r}
x <- add_predicted_draws(newdata = data.frame(TMAX = DTN.df$TMAX, Date = DTN.df$Date), 
                    fit1, n = 1000) %>%
  mutate(TMAX2est = as.integer(.prediction) + minTMAX2 - 1) %>%
  group_by(Date) %>%
  sample_n(size = 3) %>%
  pull(TMAX2est)

z <- data.frame(matrix(x, ncol = 3, byrow = FALSE))
names(z) <- c("C1", "C2", "C3")
z$Date <- DTN.df$Date

YearCount.df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) %>%
      filter(STATION == 'USC00088754') %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      select(Date, Year, Month, Day, TMAX, TMIN, PRCP) %>%
left_join(z, by = "Date") %>%
  filter(Date < as.Date("1940-03-01")) %>%
  group_by(Year) %>%
  summarize(C1 = sum(C1 >= 100, na.rm = TRUE),
            C2 = sum(C2 >= 100, na.rm = TRUE),
            C3 = sum(C3 >= 100, na.rm = TRUE),
            Uncorrected = sum(TMAX >= 100, na.rm = TRUE))
```

Read only the data from the airport site.
```{r}
TLH.df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) %>%
      filter(STATION == 'USW00093805') %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      select(Date, Year, Month, Day, TMAX, TMIN, PRCP)
```

```{r}
YearCount2.df <- TLH.df %>%
  group_by(Year) %>%
  summarize(C1 = sum(TMAX >= 100, na.rm = TRUE),
            C2 = sum(TMAX >= 100, na.rm = TRUE),
            C3 = sum(TMAX >= 100, na.rm = TRUE),
            Uncorrected = C1)

library(reshape2)
YC.df <- rbind(YearCount.df, YearCount2.df) %>%
  melt(id.vars = "Year") %>%
  mutate(variable = factor(variable, 
                           levels = c("Uncorrected", 
                                      "C1", "C2", "C3")))
levels(YC.df$variable) <- c("Raw", "Corrected 1", "Corrected 2", "Corrected 3")

ggplot(YC.df, aes(x = Year, y = value, fill = value)) + 
  geom_bar(stat='identity') + 
  scale_fill_continuous(low = 'orange', high = 'red') +
  geom_text(aes(label = value), vjust = -.5, size = 3) +
  scale_y_continuous(limits = c(0, 25), position = "right") +
  ylab("Number of Days") +
  facet_wrap(~ variable, nrow = 4, strip.position =  "bottom" ) +
  ggtitle(label = expression(paste('Number of days at or above 100', {}^o, " F")),
          subtitle = "Tallahassee, Florida, USA (1892-2018)") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 11), 
        legend.position = "none") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50"))

```




* TMAX: Daily maximum temperature, 
* TMIN: Daily minimum temperature, 
* PRCP: Daily total precipitation, 
* WDF1: Wind direction of fastest one-minute wind, 
* WSF1: Wind speed of fastest one-minute wind

Check for missing values in maximum and minimum temperature
```{r}
sum(is.na(TLH.df$TMAX))
TLH.df$Date[is.na(TLH.df$TMAX)]
TLH.df$Date[is.na(TLH.df$TMIN)]
```

Add temperatures for July 8, 2005 from Weather Underground.
```{r}
TLH.df$TMAX[TLH.df$Date == "2005-07-08"] <- 96
TLH.df$TMIN[TLH.df$Date == "2005-07-08"] <- 71
```

Line plot of the average annual daily high (low) temperature.
https://cedricscherer.netlify.com/2019/05/17/the-evolution-of-a-ggplot-ep.-1/
```{r}
library(ggplot2)

TLH.df %>% 
  filter(Year >= 1941, TMIN >= 77) %>%
  group_by(Year) %>%
  summarize(avgT = mean(TMIN, na.rm = TRUE)) %>%
#  summarize(avgP = mean(PRCP, na.rm = TRUE)) %>%
ggplot(aes(x = Year, y = avgT)) +
  geom_point(size = 3) +
  geom_line() +
  ylab(expression(paste('Temperature (', {}^o, " F)"))) +
  ggtitle(label = "Annual Average Daily High Temperature",
          subtitle = "Tallahassee, Florida, USA (1941-2018)") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50")) +
  theme_bw()
```

Last hot days?
```{r}
df %>% 
  filter(TMAX >= 100 & Month == 5) %>%
  arrange(desc(Date)) %>%
  head(n = 20)

df %>%
  arrange(desc(TMAX)) %>%
  head()
```

Hot days since 2009.
```{r}
hotDates <- df %>%
  filter(Year >= 2009 & TMAX >= 100) %>%
  select(Date) %>%
  pull()
```

Hourly data.
```{r}
dfH <- read.csv(file = "TLH_Hourly2009.csv", 
                header = TRUE,
                stringsAsFactors = FALSE)
str(dfH)
```

Report type: https://www1.ncdc.noaa.gov/pub/data/ish/ish-format-document.pdf

FM-12 = SYNOP Report of surface observation form a fixed land station
FM-15 = METAR Aviation routine weather report
FM-16 = SPECI Aviation selected special weather report

Convert the date/time character string to a data/time obect.
```{r}
library(lubridate)

dfH2 <- dfH %>%
  mutate(DateTime = ymd_hms(DATE, tz = "EST"),
         Date = as.Date(DateTime, tz = "EST"),
         TempF = as.integer(HourlyDryBulbTemperature),
         ReportType = REPORT_TYPE) %>%
  select(ReportType, DateTime, Date, TempF) %>%
  filter(Date %in% hotDates) %>%
  filter(ReportType == 'FM-15')
```

Consecutive hours at or above 100F.
```{r}
hotTimes <- rle(dfH2$TempF >= 100)
consecHours <- hotTimes$lengths[hotTimes$values]
consecHours <- consecHours[!is.na(consecHours)]

dfH2 <- dfH2 %>%
  filter(TempF >= 100) %>%
  mutate(dateNo = rep(1:length(consecHours), consecHours))
```

Join the daily data with the hourly data.
```{r}
df2 <- df %>%
  select(Date, TMAX) %>%
  filter(TMAX >= 100)

dfH2 <- left_join(dfH2, df2)
```

Round the times to nearest hour.
```{r}
dfH2 <- dfH2 %>%
  mutate(Hour =  hour(round(DateTime, "hours")))
```

### Compare with the standard model

Compare with the number of 90F days. The upward trend starts at 95F. No changes in temps lower than that. This contrasts with the standard model where we expect upward trends in all temperatures above the average temperature.
```{r}
TLH.df %>% 
#  filter(Month >= 5 & Month <= 9) %>%
  group_by(Year) %>% 
  summarize(n90 = sum(TMAX == 78)) %>% 
#  summarize(mT = mean(TMAX)) %>%
  ggplot(aes(Year, n90)) + 
  geom_point() +
  geom_smooth(method = lm)
```

Start with a random distribution of high temps using a normal density.
```{r}
TLH.df %>%
  group_by(Year < 1980) %>%
  summarize(avgHighT = mean(TMAX),
            avgLowT = mean(TMIN),
            medianHighT = median(TMAX),
            medianLowT = median(TMIN),
            sdHighT = sd(TMAX),
            sdLowT = sd(TMIN))

Random.df <- data.frame(rT = c(round(rnorm(n = 1000000, mean = 79, sd = 12.1)),
                               round(rnorm(n = 1000000, mean = 80.1, sd = 12.2))),
                        Epoch = c(rep("1940-1979", each = 1000000),
                                  rep("1980-2018", each = 1000000)))

FreqByRandomT <- Random.df %>%
  group_by(Epoch, rT) %>%
  summarize(nH = n()) %>%
  group_by(Epoch) %>%
  mutate(nDays = sum(nH),
         perc = nH/nDays)

ggplot(FreqByRandomT[FreqByRandomT$rT >= 80 & FreqByRandomT$rT <= 105 , ], aes(x = rT, y = perc, color = Epoch)) +
  geom_line(size = 1) +
  scale_y_continuous(position = "right") +
  scale_color_manual(labels = c("1940-1979", "1980-2018"), values = c("blue", "red")) +
  ylab("") +  xlab("") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) 
```

Repeat for actual temperatures.
```{r}
FreqByT <- TLH.df %>%
  mutate(Epoch = Year <= 1980) %>%
  group_by(Epoch, TMAX) %>%
  summarize(nH = n()) %>%
  group_by(Epoch) %>%
  mutate(nDays = sum(nH),
         perc = nH/nDays)

ggplot(FreqByT[FreqByT$TMAX >= 80, ], aes(x = TMAX, y = perc, color = Epoch)) +
#  geom_line() +
  stat_smooth(method = "loess", formula = y ~ x, span = .34, size = 1, se = FALSE) +
  scale_x_continuous(breaks = seq(80, 104, 2),
                     labels = paste0(seq(80, 104, 2), "°")) +
  scale_y_continuous(position = "right") +
  scale_color_manual(labels = c("1980-2018", "1940-1979"), values = c("red", "blue")) +
  ylab("") +  xlab("") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) 
```
