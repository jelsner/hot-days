---
title: "Hot Days in Tallahassee"
author: "James Elsner and Svetoslava Elsner"
date: "5/16/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

Title: Extremely hot days in Tallahassee, Florida, USA: 1940-2018

### Lede: The hots are getting hotter

We examine the statistics of extremely hot days in Tallahassee, Florida. An extremely hot day is defined as one in which the maximum temperature exceeds 100$+^\circ$ F. An extremely hot night is defined as one in which the minimum temperature fails to drop below 77$+^\circ$ F.   A record of daily maximum temperatures from the regional airport reveals an increasing trend in the occurrence of such days since the 1970s. The trend results from an increase in the occurrence of hot events and an increase in the average length of an event. We speculate that the trend is related to changes in the intensity and location of deep layer ridging, perhaps exacerbated by changes in land use and global warming.

The combination of heat and humidity is expected to reach excessive, unhealthy levels. In particular, this event is early in the season when people are less acclimated to the heat, and this is expected to be a long duration event. These factors tend to exacerbate any heat related illness. The most vulnerable populations include the elderly, homeless, and those with medical conditions, especially
where there is a lack of air conditioning. With the holiday weekend, outdoor event goers will also have an increased risk for heat related illness. There is also a larger than normal population without air conditioning in areas recovering from Hurricane Michael.

The dry air mass will allow for efficient solar heating today. Highs will be in the upper 90s and near 100 away from the immediate coast. A Heat Advisory is in effect today through Monday in the areas hit hardest by Hurricane Michael. Due to deforestation, temps may be a few degrees hotter in these locations.

Clearly, weather patterns explain this heat wave. I often cringe when people use a "cold day" to refute climate change. I also point out that "one hot" day or week doesn't necessarily affirm it either. However, there is plenty of evidence that climate change is happening so resist the urge to use one day. The 2018 National Climate Assessment report (and most studies or statements from credible science organizations) released by the Trump Administration highlighted that extreme heat will become a "new normal." For example, the figure below illustrates that the number of nighttime temperatures above 75 degrees F has increased dramatically. This is far more worrisome than daytime temperatures from the standpoint of human health according to scientific and health experts. The report points out:

Sixty-one percent of major Southeast cities are exhibiting some aspects of worsening heat waves, which is a higher percentage than any other region of the country. Hot days and warm nights together impact human comfort and health and result in the need for increased cooling efforts. Agriculture is also impacted by a lack of nighttime cooling.

High daytime temperatures can also be dangerous if strenuous activities are undertaken without proper hydration and acclimatization. 

Tropical moisture will increase as well as mid level subsidence decrease.

The more striking takeaway is that the lows are getting hotter, at a much faster rate. That's where the real public health implications bear out in the short term.

Deep layer ridging, loss of daytime heating

Compare with Albany GA

About the climate in Tallahassee.
http://weatherspark.com/history/31772/2013/Tallahassee-Florida-United-States

Need a map with inset. See file:///Users/jameselsner/Desktop/ClassNotes/appl-spat-stat/09-Lesson.html

Airport weather station site location 30.39306°, -84.35333°
Downtown weather station site location 30.43333°, -84.28333°

Create a map with an inset showing the location of Tallahassee, FL.
```{r}
library(sf)
library(dplyr)

TLH_site.df <- data.frame(name = "TLH            ", 
                          lon = -84.35333, 
                          lat = 30.39306)
TLH_site.sf <- st_as_sf(TLH_site.df, 
                        coords = c("lon", "lat"))
st_crs(TLH_site.sf) <- 4326

library(tmap)
library(USAboundaries)

USA <- us_states() %>%
  filter(name != "Hawaii" & name != "Alaska")

FL1.sf <- us_states() %>%
  filter(name == "Florida")

( inset.tm <- tm_shape(world) +
  tm_polygons() +
tm_shape(world[world$name_long == "United States", ]) +
  tm_polygons(col = "gray") +
tm_shape(USA, projection = "laea_NA", is.master = TRUE) + 
  tm_borders() +
#  tm_compass(type = "arrow", position = c("left", "bottom")) +
#  tm_scale_bar(position = c("left", "bottom"), size = .75) +
  tm_style("natural") +
tm_shape(FL1.sf) +
  tm_polygons(col = "white") )

FL.sf <- us_states(states = "Florida")
FL.sf$name <- "            Florida"
ALGA.sf <- us_states(states = c("Alabama", "Georgia"))

Leon.sf <- us_counties(states = "FL") %>%
  filter(name == "Leon")

( main.tm <- 
    tm_shape(FL.sf, projection = "laea_NA") +
       tm_polygons(col = "white") +
       tm_text(text = "name", size = 1.5) +
    tm_shape(ALGA.sf) +
       tm_polygons(col = "gray") +
    tm_shape(Leon.sf) +
       tm_polygons() +
    tm_shape(TLH_site.sf) +
       tm_symbols(shape = 21, size = .1, col = NA) +
 #      tm_text(text = "name") + 
    tm_compass(type = "arrow", position = c("right", "top")) +
    tm_scale_bar(position = c("right", "top"), size = .75) +
    tm_style("natural") )

library(grid)
main.tm
print(inset.tm, vp = viewport(.3, .3, width = .7, height = .4))
```
###Figure: Location map. Tallahassee.

Get the data. Ordered online from 
https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00093805/detail

TALLAHASSEE REGIONAL AIRPORT, FL US (Station ID: GHCND:USW00093805)
TALLAHASSEE, FL US (Station ID: GHCND:USC00088754)

Do the same for Sofia Bulgaria
https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:BUM00015614/detail

https://docs.opendata.aws/noaa-ghcn-pds/readme.html

Global Historical Climate Network includes daily land surface observations from around the world. The GHCN-Daily was developed to meet the needs of climate analysis and monitoring studies that require data at a sub-monthly time resolution (e.g., assessments of the frequency of heavy rainfall, heat wave duration, etc.). The dataset includes observations from World Meteorological Organization, Cooperative, and CoCoRaHS networks. If observed, the station dataset includes max and minimum temperatures, total precipitation, snowfall, and depth of snow on ground. Some U.S. station data are typically delayed only 24 hours.

## Part 1: Analysis of hot days and nights (1940-2018)

The airport record begins March 1, 1940. Since we are only interested in temperatures that occur during late spring and summer the missing months of Jan and Feb in 1940 do not influence any of the key aspects of the present analysis.

Get data.
```{r}
library(lubridate)

TLH.df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) %>%
      filter(STATION == 'USW00093805') %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), 
             Month = month(Date), 
             Day = day(Date),
             doy = yday(Date)) %>%
      select(Date, Year, Month, Day, doy, TMAX, TMIN, PRCP)
```

Histogram of daily high temperature.
```{r}
FreqByT <- TLH.df %>%
  group_by(TMAX) %>%
  summarize(nH = n())
FreqByT2 <-FreqByT %>%
  filter(TMAX >= 100)

( p1 <- ggplot(FreqByT, aes(x = TMAX, y = nH)) +
  geom_bar(stat = 'identity', col = 'white', fill = "gray70") +
  geom_bar(data = FreqByT2, stat = 'identity', fill = "orange") +
  scale_x_continuous(breaks = seq(40, 100, 10),
                     labels = paste0(seq(40, 100, 10), "°")) +
  scale_y_continuous(position = "right", limits = c(0, 1600)) +
  ylab("") +  xlab("") +
#  ggtitle(label = "Frequency of Daily High Temperatures",
#          subtitle = "Tallahassee, FL, USA (1940-2018)") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) )

FreqByT <- TLH.df %>%
  group_by(TMIN) %>%
  summarize(nH = n())
FreqByT2 <-FreqByT %>%
  filter(TMIN >= 77)

(p2 <- ggplot(FreqByT, aes(x = TMIN, y = nH)) +
  geom_bar(stat = 'identity', col = 'white', fill = "gray70") +
  geom_bar(data = FreqByT2, stat = 'identity', fill = "orange") +
  scale_x_continuous(breaks = seq(20, 80, 10),
                     labels = paste0(seq(20, 80, 10), "°")) +
  scale_y_continuous(position = "right", limits = c(0, 1600)) +
  ylab("") + xlab("") +
#  ggtitle(label = "Frequency of Daily Low Temperatures",
#          subtitle = "Tallahassee, FL, USA (1940-2018)") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) )

library(ggpubr)

ggarrange(p1, p2, 
          nrow = 2, 
          labels = c("Number of Days by Maximum Temperature", 
                     "Number of Days by Minimum Temperature"),
          font.label = list(face = "plain", size = 11))
```

Define a hot day/night.

How many hot days? Monthly counts. Median date.
```{r}
TLH.df %>%
  summarize(N100 = sum(TMAX >= 100, na.rm = TRUE),
            N77 = sum(TMIN >= 77, na.rm = TRUE))

TLH.df %>%
  group_by(Month) %>%
  summarize(N100 = sum(TMAX >= 100, na.rm = TRUE),
            N77 = sum(TMIN >= 77, na.rm = TRUE)) %>%
  filter(Month %in% 5:10) %>%
  mutate(MonthName = month.name[Month]) %>%
  select(MonthName, N100, N77)

TLH.df %>%
  filter(TMAX >= 100) %>%
  summarize(medianDate = median(doy))

# 182: July 1st
TLH.df %>%
  filter(TMIN >= 77) %>%
  summarize(medianDate = median(doy))
# 216: August 4th

library(reshape2)

DOY <- TLH.df %>%
  group_by(doy) %>%
  summarize("Number of Times the Daily High Reached 100° F" = sum(TMAX >= 100, na.rm = TRUE),
            "Number of Times the Daily Low Remained Above 77° F" = sum(TMIN >= 77, na.rm = TRUE)) %>%
  filter(doy %in% 139:250) %>%
  melt(id.vars = "doy")

ggplot(DOY, aes(x = doy, y = value)) +
  geom_bar(stat = 'identity', col = "white", fill = "orange") +
  scale_x_continuous(position = "bottom", 
                     breaks = c(152, 166, 182, 196, 213, 227, 244),
                     labels = c("June 1", "June 15", 
                                "July 1", "July 15", 
                                "August 1", "August 15",
                                "September 1")) +
  scale_y_continuous(position = "left") +
  facet_wrap(~ variable, nrow = 2, strip.position = "top") +
  ylab("") + xlab("") +
#  ggtitle(label = "Number of Extremely Hot Days & Nights By Day of Year",
#          subtitle = "Tallahassee, Florida, USA (1940-2018)") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.y = element_blank(),
        strip.text.x = element_text(size = 11),
        legend.position = "none") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50")) 
```

Bar plot of the number of 100+ degree days.
```{r}
TLH.df %>% 
  group_by(Year) %>%
  summarize("High Temperature At Or Above 100°"  = sum(TMAX >= 100, na.rm = TRUE),
            "Low Temperature At Or Above 77°" = sum(TMIN >= 77, na.rm = TRUE)) %>%
  melt(id.vars = "Year") %>%
ggplot(aes(x = Year, y = value, fill = value)) + 
  geom_bar(stat='identity') + 
  scale_fill_continuous(low = 'orange', high = 'red') +
  scale_x_continuous(position = "top", breaks = seq(1945, 2015, by = 10)) +
  scale_y_continuous(limits = c(0, 25), position = "right") +
#  geom_text(aes(label = value), vjust = -.5, size = 2.5) +
  facet_wrap(~ variable, nrow = 2, strip.position = "bottom") +
  ylab("") + xlab("") +
  ggtitle(label = "Number of Extremely Hot Days & Nights",
          subtitle = "Tallahassee, Florida, USA (1940-2018)") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        strip.text.x = element_text(size = 11),
        legend.position = "none") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50")) 
```

### Analysis of temperatures when data are available at both sites.
```{r}
df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) 

duplicateRows <- duplicated(df$Date)
duplicateDates <- df$Date[duplicateRows]

df1 <- df %>%
  filter(Date %in% duplicateDates) %>%
  filter(STATION == "USC00088754")

df2 <- df %>%
  filter(Date %in% duplicateDates) %>%
  filter(STATION == "USW00093805") %>%
  select(STATION2 = STATION, TMAX2 = TMAX, TMIN2 = TMIN, PRCP2 = PRCP)

dfOverlap <- cbind(df1, df2)
```

```{r}
dfOverlap <- dfOverlap %>%
  mutate(OLDwarmer = TMAX - TMAX2 > 0,
         OLDcolder = TMAX - TMAX2 < 0)

sum(dfOverlap$OLDwarmer[dfOverlap$TMIN >= 78], na.rm = TRUE)
sum(dfOverlap$OLDcolder[dfOverlap$TMIN >= 78], na.rm = TRUE)

mean(dfOverlap$TMAX, na.rm = TRUE)
mean(dfOverlap$TMAX2, na.rm = TRUE)

mean(dfOverlap$TMIN, na.rm = TRUE)
mean(dfOverlap$TMIN2, na.rm = TRUE)

mean(dfOverlap$TMAX[dfOverlap$TMAX >= 95], na.rm = TRUE)
mean(dfOverlap$TMAX2[dfOverlap$TMAX >= 95], na.rm = TRUE)

mean(dfOverlap$TMAX[dfOverlap$TMAX >= 100], na.rm = TRUE)
mean(dfOverlap$TMAX2[dfOverlap$TMAX >= 100], na.rm = TRUE)

```
The overall daily high temperature means are nearly identical. For hot days the difference in average highs is in favor of downtown.

Daily temperatures in Tallahassee were recorded from a station downtown until the early 1940s. Since then at the airport. Avg difference in highs during the overlap period is small (less than 1/2°F). But it is much more likely to get to 100°F+ downtown. 

Cumulative logistic regression model. Note: The model cannot predict temperatures above 100 at the airport since there were none in the record during the overlapping years. It can assign an exceedance probability for a larger range of input temperatures.
```{r}
library(brms)

minTMAX2 <- min(dfModel2$TMAX2)

dfModel2 <- dfModel %>%
  filter(TMAX >= 94) %>%
  mutate(TI = TMAX2 - minTMAX2 + 1,
         TDT = scale(TMAX))

family <- brms::cumulative(threshold = "equidistant")
formula <- TI ~ 1

get_prior(formula, data = dfModel2, family = family)

fit0 <- brm(formula = formula,
           data = dfModel2,
           family = family,
           prior = set_prior("student_t(3, 0, 10)", class = "Intercept"),
           seed = 9121)
fixef(fit0)

fit0_out <- predict(fit0, probs = c(0, 1))
head(fit0_out)

formula <- TI ~ TMAX

get_prior(formula, data = dfModel2, family = family)

fit1 <- brm(formula = formula,
            data = dfModel2,
            family = family,
            prior = c(set_prior("normal(0, 1)", class = "b"),
                     set_prior("student_t(7, 0, 10)", class = "Intercept")),
           seed = 78121)

summary(fit1)

library(tidybayes)
library(modelr)

# initialize out data frame
out <- dfModel2 %>%
  data_grid(TMAX = 96:106) %>%
  add_predicted_draws(fit1, n = 1000) %>%
  mutate(pTMAX2 = as.integer(.prediction)  + minTMAX2 - 1) %>%
  group_by(TMAX) %>%
  summarize(nTMAX = n(),
            prob100 = sum(pTMAX2 >= 100)/nTMAX) 

out

# rbind to the initialized data frame
for(i in 2:100){
  out <- dfModel2 %>%
  data_grid(TMAX = 96:106) %>%
  add_predicted_draws(fit1, n = 1000) %>%
  mutate(pTMAX2 = as.integer(.prediction)  + minTMAX2 - 1) %>%
  group_by(TMAX) %>%
  summarize(nTMAX = n(),
            prob100 = sum(pTMAX2 >= 100)/nTMAX) %>%
    rbind(out)
}

out$I <- rep(1:100, each = 11)

library(ggridges)

ggplot(out, aes(x = TMAX, y = prob100, group = I)) +
  geom_point(alpha = .2, size = .5) +
  scale_x_continuous(breaks = 96:106) +
  xlab("High Temperature Downtown Tallahassee (°F)") +
  ylab("Posterior Probability") +
  ggtitle("Chance the TLH Airport Reports a 100+°F Day",
          subtitle = "Given specific high temperatures downtown") +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank())
```

Read only the data from the downtown site. Filter by days at or above 96F then use the model to predict the temperature at the airport. August 31, 1895 TMAX = 2768 is removed.
```{r}
DTN.df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) %>%
      filter(STATION == 'USC00088754') %>%
      filter(TMAX >= 96) %>%
      filter(TMAX != 2768) %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      select(Date, Year, Month, Day, TMAX, TMIN, PRCP) 
table(DTN.df$TMAX)
```

Predictions. What is the likely number of 100 degree days at the airport over the period of record when the daily high temperature was recorded downtown?
```{r}
n100 <- numeric()
for(i in 1:100){
TMAX2 <- add_predicted_draws(newdata = data.frame(TMAX = DTN.df$TMAX, Date = DTN.df$Date), 
                    fit1, n = 1000) %>%
  mutate(TMAX2est = as.integer(.prediction) + minTMAX2 - 1) %>%
  group_by(Date) %>%
  sample_n(size = 1) %>%
  pull(TMAX2est) 
n100 <- c(n100, sum(TMAX2 >= 100))
}
```

Predictions. No loop. Look at three realizations from the model. Join predictions data frame with the original downtown data frame.
```{r}
x <- add_predicted_draws(newdata = data.frame(TMAX = DTN.df$TMAX, Date = DTN.df$Date), 
                    fit1, n = 1000) %>%
  mutate(TMAX2est = as.integer(.prediction) + minTMAX2 - 1) %>%
  group_by(Date) %>%
  sample_n(size = 3) %>%
  pull(TMAX2est)

z <- data.frame(matrix(x, ncol = 3, byrow = FALSE))
names(z) <- c("C1", "C2", "C3")
z$Date <- DTN.df$Date

YearCount.df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) %>%
      filter(STATION == 'USC00088754') %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      select(Date, Year, Month, Day, TMAX, TMIN, PRCP) %>%
left_join(z, by = "Date") %>%
  filter(Date < as.Date("1940-03-01")) %>%
  group_by(Year) %>%
  summarize(C1 = sum(C1 >= 100, na.rm = TRUE),
            C2 = sum(C2 >= 100, na.rm = TRUE),
            C3 = sum(C3 >= 100, na.rm = TRUE),
            Uncorrected = sum(TMAX >= 100, na.rm = TRUE))
```

Read only the data from the airport site.
```{r}
TLH.df <- read.csv(file = 'TLH_SOD1892.csv',
               stringsAsFactors = FALSE,
               header = TRUE) %>%
      filter(STATION == 'USW00093805') %>%
      mutate(Date = as.Date(DATE)) %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
      select(Date, Year, Month, Day, TMAX, TMIN, PRCP)
```

```{r}
YearCount2.df <- TLH.df %>%
  group_by(Year) %>%
  summarize(C1 = sum(TMAX >= 100, na.rm = TRUE),
            C2 = sum(TMAX >= 100, na.rm = TRUE),
            C3 = sum(TMAX >= 100, na.rm = TRUE),
            Uncorrected = C1)

library(reshape2)
YC.df <- rbind(YearCount.df, YearCount2.df) %>%
  melt(id.vars = "Year") %>%
  mutate(variable = factor(variable, 
                           levels = c("Uncorrected", 
                                      "C1", "C2", "C3")))
levels(YC.df$variable) <- c("Raw", "Corrected 1", "Corrected 2", "Corrected 3")

ggplot(YC.df, aes(x = Year, y = value, fill = value)) + 
  geom_bar(stat='identity') + 
  scale_fill_continuous(low = 'orange', high = 'red') +
  geom_text(aes(label = value), vjust = -.5, size = 3) +
  scale_y_continuous(limits = c(0, 25), position = "right") +
  ylab("Number of Days") +
  facet_wrap(~ variable, nrow = 4, strip.position =  "bottom" ) +
  ggtitle(label = expression(paste('Number of days at or above 100', {}^o, " F")),
          subtitle = "Tallahassee, Florida, USA (1892-2018)") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 11), 
        legend.position = "none") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50"))

```
* TMAX: Daily maximum temperature, 
* TMIN: Daily minimum temperature, 
* PRCP: Daily total precipitation, 
* WDF1: Wind direction of fastest one-minute wind, 
* WSF1: Wind speed of fastest one-minute wind

Check for missing values in maximum and minimum temperature
```{r}
sum(is.na(TLH.df$TMAX))
TLH.df$Date[is.na(TLH.df$TMAX)]
TLH.df$Date[is.na(TLH.df$TMIN)]
```

Add temperatures for July 8, 2005 from Weather Underground.
```{r}
TLH.df$TMAX[TLH.df$Date == "2005-07-08"] <- 96
TLH.df$TMIN[TLH.df$Date == "2005-07-08"] <- 71
```

Line plot of the average annual daily high (low) temperature.
https://cedricscherer.netlify.com/2019/05/17/the-evolution-of-a-ggplot-ep.-1/
```{r}
library(ggplot2)

TLH.df %>% 
  filter(Year >= 1941, TMIN >= 77) %>%
  group_by(Year) %>%
  summarize(avgT = mean(TMIN, na.rm = TRUE)) %>%
#  summarize(avgP = mean(PRCP, na.rm = TRUE)) %>%
ggplot(aes(x = Year, y = avgT)) +
  geom_point(size = 3) +
  geom_line() +
  ylab(expression(paste('Temperature (', {}^o, " F)"))) +
  ggtitle(label = "Annual Average Daily High Temperature",
          subtitle = "Tallahassee, Florida, USA (1941-2018)") +
  labs(caption = "Data: Global Historical Climate Network") +
  theme(plot.caption = element_text(size = 9, color = "grey50")) +
  theme_bw()
```

Last hot days?
```{r}
df %>% 
  filter(TMAX >= 100 & Month == 5) %>%
  arrange(desc(Date)) %>%
  head(n = 20)

df %>%
  arrange(desc(TMAX)) %>%
  head()
```

Hot days since 2009.
```{r}
hotDates <- df %>%
  filter(Year >= 2009 & TMAX >= 100) %>%
  select(Date) %>%
  pull()
```

Hourly data.
```{r}
dfH <- read.csv(file = "TLH_Hourly2009.csv", 
                header = TRUE,
                stringsAsFactors = FALSE)
str(dfH)
```

Report type: https://www1.ncdc.noaa.gov/pub/data/ish/ish-format-document.pdf

FM-12 = SYNOP Report of surface observation form a fixed land station
FM-15 = METAR Aviation routine weather report
FM-16 = SPECI Aviation selected special weather report

Convert the date/time character string to a data/time obect.
```{r}
library(lubridate)

dfH2 <- dfH %>%
  mutate(DateTime = ymd_hms(DATE, tz = "EST"),
         Date = as.Date(DateTime, tz = "EST"),
         TempF = as.integer(HourlyDryBulbTemperature),
         ReportType = REPORT_TYPE) %>%
  select(ReportType, DateTime, Date, TempF) %>%
  filter(Date %in% hotDates) %>%
  filter(ReportType == 'FM-15')
```

Consecutive hours at or above 100F.
```{r}
hotTimes <- rle(dfH2$TempF >= 100)
consecHours <- hotTimes$lengths[hotTimes$values]
consecHours <- consecHours[!is.na(consecHours)]

dfH2 <- dfH2 %>%
  filter(TempF >= 100) %>%
  mutate(dateNo = rep(1:length(consecHours), consecHours))
```

Join the daily data with the hourly data.
```{r}
df2 <- df %>%
  select(Date, TMAX) %>%
  filter(TMAX >= 100)

dfH2 <- left_join(dfH2, df2)
```

Round the times to nearest hour.
```{r}
dfH2 <- dfH2 %>%
  mutate(Hour =  hour(round(DateTime, "hours")))
```